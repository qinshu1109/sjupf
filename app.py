#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æŠ–éŸ³ç”µå•†æ•°æ®æ¸…æ´—å·¥å…·
ä¸“ä¸šçš„Excelæ•°æ®æ ‡å‡†åŒ–å¤„ç†åº”ç”¨

æŠ€æœ¯æ ˆï¼šStreamlit + pandas + openpyxl
ä½œè€…ï¼šæ•°æ®æ¸…æ´—å·¥ç¨‹å¸ˆ
"""

import streamlit as st
import pandas as pd
import numpy as np
import os
import zipfile
import tempfile
import re
from io import BytesIO
from datetime import datetime
from typing import Dict, List, Tuple, Optional
import traceback

# é¡µé¢é…ç½®
st.set_page_config(
    page_title="æŠ–éŸ³æ•°æ®æ¸…æ´—å·¥å…·",
    page_icon="ğŸ§¹",
    layout="wide",
    initial_sidebar_state="expanded"
)

# æ ‡å‡†å­—æ®µå®šä¹‰
STANDARD_FIELDS = {
    'product_name': 'å•†å“åç§°',
    'product_url': 'å•†å“é“¾æ¥',
    'category_l1': 'ä¸€çº§åˆ†ç±»',
    'commission': 'ä½£é‡‘æ¯”ä¾‹',
    'sales_7d': '7å¤©é”€é‡',
    'gmv_7d': '7å¤©GMV',
    'sales_30d': '30å¤©é”€é‡',
    'gmv_30d': '30å¤©GMV',
    'live_gmv_30d': '30å¤©ç›´æ’­GMV',
    'card_gmv_30d': '30å¤©å•†å“å¡GMV',
    'sales_1y': '1å¹´é”€é‡',
    'conv_30d': '30å¤©è½¬åŒ–ç‡',
    'rank_type': 'æ¦œå•ç±»å‹',
    'rank_no': 'æ’å',
    'influencer_7d': '7å¤©å¸¦è´§è¾¾äºº',
    'snapshot_tag': 'æ•°æ®å¿«ç…§æ ‡ç­¾',
    'source_table': 'æ•°æ®æ¥æºè¡¨',
    'file_date': 'æ–‡ä»¶æ—¥æœŸ',
    'data_period': 'æ•°æ®å‘¨æœŸ'
}

# å­—æ®µåˆ«åæ˜ å°„
FIELD_ALIASES = {
    'product_name': ['å•†å“', 'å•†å“åç§°', 'äº§å“å', 'å•†å“æ ‡é¢˜', 'åç§°'],
    'product_url': ['å•†å“é“¾æ¥', 'æŠ–éŸ³å•†å“é“¾æ¥', 'é“¾æ¥', 'URL'],
    'category_l1': ['å•†å“åˆ†ç±»', 'åˆ†ç±»', 'ä¸€çº§åˆ†ç±»', 'ç±»ç›®'],
    'commission': ['ä½£é‡‘æ¯”ä¾‹', 'ä½£é‡‘', 'ææˆæ¯”ä¾‹', 'åˆ†æˆ'],
    'sales_7d': ['å‘¨é”€é‡', 'è¿‘7å¤©é”€é‡', '7å¤©é”€é‡', '7æ—¥é”€é‡'],
    'gmv_7d': ['å‘¨é”€å”®é¢', 'è¿‘7å¤©é”€å”®é¢', '7å¤©GMV', '7æ—¥GMV'],
    'sales_30d': ['è¿‘30å¤©é”€é‡', '30å¤©é”€é‡', 'æœˆé”€é‡'],
    'gmv_30d': ['è¿‘30å¤©é”€å”®é¢', '30å¤©é”€å”®é¢', 'æœˆé”€å”®é¢'],
    'live_gmv_30d': ['30å¤©ç›´æ’­GMV', 'ç›´æ’­GMV', 'ç›´æ’­é”€å”®é¢', 'è¿‘30å¤©ç›´æ’­é”€å”®é¢'],
    'card_gmv_30d': ['30å¤©å•†å“å¡GMV', 'å•†å“å¡GMV', 'å•†å“å¡é”€å”®é¢', 'è¿‘30å¤©å•†å“å¡é”€å”®é¢'],  # ä¿®å¤ï¼šæ·»åŠ ç¼ºå¤±çš„å­—æ®µæ˜ å°„
    'sales_1y': ['1å¹´é”€é‡', 'è¿‘1å¹´é”€é‡', 'å¹´é”€é‡', '1å¹´é”€å”®é‡'],  # ä¿®å¤ï¼šæ·»åŠ ç¼ºå¤±çš„å­—æ®µæ˜ å°„
    'conv_30d': ['30å¤©è½¬åŒ–ç‡', 'è½¬åŒ–ç‡', 'è½¬æ¢ç‡'],
    'rank_no': ['æ’å', 'æ’è¡Œ', 'åæ¬¡'],
    'influencer_7d': ['å‘¨å¸¦è´§è¾¾äºº', 'å…³è”è¾¾äºº', 'å¸¦è´§è¾¾äºº', 'è¾¾äºº']
}


def numeric_normalizer(df: pd.DataFrame) -> pd.DataFrame:
    """
    æ•°å€¼æ ¼å¼æ ‡å‡†åŒ–å¤„ç†å‡½æ•°

    å¤„ç†å„ç§æ•°å€¼æ ¼å¼å¹¶è½¬æ¢ä¸ºæ ‡å‡†æ•°å€¼ï¼š
    - åŒºé—´å€¼ï¼š"7.5w-10w" â†’ 87500 (å–ä¸­ä½æ•°)
    - ç™¾åˆ†æ¯”ï¼š"20%" â†’ 0.2
    - ä¸­æ–‡æ•°å€¼ï¼š"1.2ä¸‡" â†’ 12000
    - ç©ºå€¼æ ‡è®°ï¼š"â€”"ã€"æ— æ•°æ®" â†’ NaN

    å‚æ•°:
        df: å¾…å¤„ç†çš„DataFrame

    è¿”å›:
        å¤„ç†åçš„DataFrame
    """
    import re

    # éœ€è¦å¤„ç†çš„æ•°å€¼å­—æ®µ
    numeric_fields = ['commission', 'sales_7d', 'gmv_7d', 'sales_30d', 'gmv_30d',
                     'live_gmv_30d', 'card_gmv_30d', 'sales_1y', 'conv_30d']

    df_processed = df.copy()

    for field in numeric_fields:
        if field not in df_processed.columns:
            continue

        # åˆ›å»ºæ–°çš„Seriesç”¨äºå­˜å‚¨å¤„ç†ç»“æœ
        processed_series = df_processed[field].copy()

        for idx in processed_series.index:
            val = str(processed_series.iloc[idx])

            # è·³è¿‡å·²ç»æ˜¯NaNçš„å€¼
            if val in ['nan', 'None', ''] or pd.isna(processed_series.iloc[idx]):
                processed_series.iloc[idx] = pd.NA
                continue

            # å¤„ç†ç©ºå€¼æ ‡è®°
            if val in ['â€”', 'æ— æ•°æ®']:
                processed_series.iloc[idx] = pd.NA
                continue

            # å¤„ç†ç™¾åˆ†æ¯”æ ¼å¼ (å¦‚: "20%" â†’ 0.2)
            if '%' in val:
                try:
                    num_val = float(val.replace('%', '').replace(',', ''))
                    processed_series.iloc[idx] = num_val / 100
                    continue
                except:
                    pass

            # å¤„ç†åŒºé—´å€¼ (å¦‚: "7.5w-10w" â†’ 87500)
            if '-' in val and re.search(r'\d+\.?\d*[wWä¸‡åƒ]?-\d+\.?\d*[wWä¸‡åƒ]?', val):
                try:
                    numbers = re.findall(r'(\d+\.?\d*)([wWä¸‡åƒ]?)', val)
                    if len(numbers) >= 2:
                        vals = []
                        for num, unit in numbers[:2]:
                            num_val = float(num)
                            if unit.lower() in ['w', 'ä¸‡']:
                                num_val *= 10000
                            elif unit in ['åƒ']:
                                num_val *= 1000
                            vals.append(num_val)
                        processed_series.iloc[idx] = sum(vals) / 2
                        continue
                except:
                    pass

            # å¤„ç†ä¸­æ–‡æ•°å€¼å•ä½ (å¦‚: "1.2ä¸‡" â†’ 12000)
            if re.search(r'\d+\.?\d*[ä¸‡åƒwW]', val):
                try:
                    match = re.search(r'(\d+\.?\d*)([ä¸‡åƒwW])', val)
                    if match:
                        num, unit = match.groups()
                        num_val = float(num)
                        if unit in ['ä¸‡', 'w', 'W']:
                            num_val *= 10000
                        elif unit == 'åƒ':
                            num_val *= 1000
                        processed_series.iloc[idx] = num_val
                        continue
                except:
                    pass

            # å¤„ç†æ™®é€šæ•°å€¼ï¼ˆæ¸…ç†é€—å·ï¼‰
            try:
                clean_val = val.replace(',', '')
                processed_series.iloc[idx] = float(clean_val)
            except:
                # æ— æ³•è½¬æ¢çš„ä¿æŒåŸå€¼
                pass

        # æ›´æ–°DataFrame
        df_processed[field] = processed_series

    return df_processed


def smart_csv_reader(uploaded_file, nrows=None) -> pd.DataFrame:
    """
    æ™ºèƒ½CSVæ–‡ä»¶è¯»å–å™¨ï¼Œè‡ªåŠ¨æ£€æµ‹åˆ†éš”ç¬¦å’Œç¼–ç 

    å‚æ•°:
        uploaded_file: ä¸Šä¼ çš„CSVæ–‡ä»¶
        nrows: è¯»å–çš„è¡Œæ•°ï¼ŒNoneè¡¨ç¤ºè¯»å–å…¨éƒ¨

    è¿”å›:
        æˆåŠŸè§£æçš„DataFrame

    å¼‚å¸¸:
        æŠ›å‡ºè¯¦ç»†çš„CSVè§£æé”™è¯¯ä¿¡æ¯
    """
    # å¸¸è§çš„åˆ†éš”ç¬¦åˆ—è¡¨ï¼ŒæŒ‰ä¼˜å…ˆçº§æ’åº
    separators = [',', ';', '\t', '|']
    # å¸¸è§çš„ç¼–ç åˆ—è¡¨ï¼ŒæŒ‰ä¼˜å…ˆçº§æ’åº
    encodings = ['utf-8', 'gbk', 'iso-8859-1', 'cp1252']

    # é‡ç½®æ–‡ä»¶æŒ‡é’ˆ
    uploaded_file.seek(0)

    # é¦–å…ˆæ£€æŸ¥æ–‡ä»¶æ˜¯å¦ä¸ºç©º
    file_content = uploaded_file.read()
    if len(file_content) == 0:
        raise Exception("CSVæ–‡ä»¶ä¸ºç©º")

    # é‡ç½®æ–‡ä»¶æŒ‡é’ˆ
    uploaded_file.seek(0)

    best_result = None
    best_score = 0

    # å°è¯•ä¸åŒçš„ç¼–ç å’Œåˆ†éš”ç¬¦ç»„åˆ
    for encoding in encodings:
        for separator in separators:
            try:
                uploaded_file.seek(0)  # é‡ç½®æ–‡ä»¶æŒ‡é’ˆ
                df = pd.read_csv(
                    uploaded_file,
                    sep=separator,
                    encoding=encoding,
                    dtype=str,
                    nrows=nrows
                )

                # æ£€æŸ¥æ˜¯å¦æˆåŠŸè§£æå‡ºåˆ—
                if len(df.columns) > 0:
                    # è®¡ç®—è§£æè´¨é‡åˆ†æ•°
                    score = len(df.columns)  # åˆ—æ•°è¶Šå¤šè¶Šå¥½

                    # æ£€æŸ¥åˆ—åè´¨é‡
                    valid_columns = [col for col in df.columns if isinstance(col, str) and len(str(col).strip()) > 0]
                    if len(valid_columns) == 0:
                        continue

                    # å¦‚æœåªæœ‰ä¸€åˆ—ï¼Œæ£€æŸ¥æ˜¯å¦åŒ…å«å…¶ä»–åˆ†éš”ç¬¦ï¼ˆè¯´æ˜åˆ†éš”ç¬¦é”™è¯¯ï¼‰
                    if len(df.columns) == 1:
                        first_col = str(df.columns[0])
                        other_seps = [s for s in separators if s != separator]
                        if any(sep in first_col for sep in other_seps):
                            score = 0  # é™ä½åˆ†æ•°

                    # å¦‚æœæœ‰æ•°æ®è¡Œï¼Œæ£€æŸ¥æ•°æ®è´¨é‡
                    if len(df) > 0 and nrows != 0:
                        # æ£€æŸ¥ç¬¬ä¸€è¡Œæ•°æ®æ˜¯å¦åˆç†åˆ†éš”
                        first_row = df.iloc[0]
                        non_empty_cells = sum(1 for cell in first_row if pd.notna(cell) and str(cell).strip())
                        score += non_empty_cells  # éç©ºå•å…ƒæ ¼è¶Šå¤šè¶Šå¥½

                    # æ›´æ–°æœ€ä½³ç»“æœ
                    if score > best_score:
                        best_score = score
                        best_result = df

            except Exception:
                continue  # å°è¯•ä¸‹ä¸€ä¸ªç»„åˆ

    # è¿”å›æœ€ä½³ç»“æœ
    if best_result is not None and best_score > 0:
        return best_result

    # å¦‚æœæ‰€æœ‰ç»„åˆéƒ½å¤±è´¥ï¼Œæä¾›è¯¦ç»†çš„é”™è¯¯ä¿¡æ¯
    raise Exception(
        "æ— æ³•è§£æCSVæ–‡ä»¶ã€‚å¯èƒ½çš„åŸå› ï¼š\n"
        "1. æ–‡ä»¶åˆ†éš”ç¬¦ä¸æ˜¯å¸¸è§æ ¼å¼ï¼ˆé€—å·ã€åˆ†å·ã€åˆ¶è¡¨ç¬¦ã€ç«–çº¿ï¼‰\n"
        "2. æ–‡ä»¶ç¼–ç ä¸è¢«æ”¯æŒ\n"
        "3. æ–‡ä»¶æ ¼å¼æŸåæˆ–åŒ…å«ç‰¹æ®Šå­—ç¬¦\n"
        "å»ºè®®ï¼šè¯·ç¡®ä¿CSVæ–‡ä»¶ä½¿ç”¨é€—å·åˆ†éš”ï¼ŒUTF-8ç¼–ç ä¿å­˜"
    )


def clean_duplicate_columns(df: pd.DataFrame, context: str = "") -> pd.DataFrame:
    """
    æ¸…ç†DataFrameä¸­çš„é‡å¤åˆ—å

    å‚æ•°:
        df: å¾…æ¸…ç†çš„DataFrame
        context: ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œç”¨äºæ—¥å¿—è®°å½•

    è¿”å›:
        æ¸…ç†åçš„DataFrame
    """
    if df.columns.duplicated().any():
        duplicated_cols = df.columns[df.columns.duplicated()].tolist()
        print(f"è­¦å‘Š: {context} å‘ç°é‡å¤åˆ—å {duplicated_cols}ï¼Œå°†ä¿ç•™ç¬¬ä¸€ä¸ªå‡ºç°çš„åˆ—")

        # å»é™¤é‡å¤åˆ—ï¼Œä¿ç•™ç¬¬ä¸€ä¸ª
        df_cleaned = df.loc[:, ~df.columns.duplicated()]

        print(f"å»é‡å‰åˆ—æ•°: {len(df.columns)}, å»é‡ååˆ—æ•°: {len(df_cleaned.columns)}")
        return df_cleaned

    return df


def smart_field_mapping(columns: List[str]) -> Dict[str, str]:
    """
    æ™ºèƒ½å­—æ®µæ˜ å°„ï¼šè‡ªåŠ¨åŒ¹é…åŸå§‹å­—æ®µååˆ°æ ‡å‡†å­—æ®µåï¼Œé˜²æ­¢é‡å¤æ˜ å°„

    å‚æ•°:
        columns: åŸå§‹åˆ—ååˆ—è¡¨

    è¿”å›:
        æ˜ å°„å­—å…¸ {åŸå§‹å­—æ®µå: æ ‡å‡†å­—æ®µå}
    """
    mapping = {}
    used_std_fields = set()  # è·Ÿè¸ªå·²ä½¿ç”¨çš„æ ‡å‡†å­—æ®µ

    # ç¬¬ä¸€è½®ï¼šç²¾ç¡®åŒ¹é…ï¼ˆä¼˜å…ˆçº§æœ€é«˜ï¼‰
    for std_field, aliases in FIELD_ALIASES.items():
        if std_field in used_std_fields:
            continue

        for col in columns:
            if col in aliases:
                mapping[col] = std_field
                used_std_fields.add(std_field)
                break

    # ç¬¬äºŒè½®ï¼šæ¨¡ç³ŠåŒ¹é…ï¼ˆä¼˜å…ˆçº§è¾ƒä½ï¼‰
    for std_field, aliases in FIELD_ALIASES.items():
        if std_field in used_std_fields:
            continue

        for col in columns:
            if col in mapping:  # è·³è¿‡å·²æ˜ å°„çš„åŸå§‹å­—æ®µ
                continue

            # æ¨¡ç³ŠåŒ¹é…
            for alias in aliases:
                if alias in col or col in alias:
                    mapping[col] = std_field
                    used_std_fields.add(std_field)
                    break

            if std_field in used_std_fields:
                break

    return mapping


def extract_metadata_from_filename(filename: str) -> Dict[str, str]:
    """
    ä»æ–‡ä»¶åæå–å…ƒæ•°æ®ä¿¡æ¯ï¼Œæ”¯æŒå¤šç§æ—¶é—´æ ¼å¼

    æ”¯æŒçš„æ—¶é—´æ ¼å¼ï¼š
    - æ—¥æœŸåŒºé—´: YYYYMMDD-YYYYMMDD (å¦‚: 20250622-20250630)
    - å•æ—¥æœŸ: YYYYMMDD (å¦‚: 20250622)
    - ç›¸å¯¹æ—¶é—´: æ•°å­—+æ—¶é—´å•ä½ (å¦‚: 30d, 7d, 1y)

    å‚æ•°:
        filename: æ–‡ä»¶å

    è¿”å›:
        åŒ…å«å…ƒæ•°æ®çš„å­—å…¸
    """
    file_date = 'æœªçŸ¥æ—¶é—´'
    data_period = 'æœªçŸ¥å‘¨æœŸ'

    # 1. ä¼˜å…ˆåŒ¹é…æ—¥æœŸåŒºé—´æ ¼å¼ (YYYYMMDD-YYYYMMDD)
    date_range_pattern = r'(\d{8})-(\d{8})'
    date_range_match = re.search(date_range_pattern, filename)
    if date_range_match:
        start_date = date_range_match.group(1)
        end_date = date_range_match.group(2)
        # æ ¼å¼åŒ–ä¸ºå¯è¯»çš„æ—¥æœŸåŒºé—´
        file_date = f"{start_date[:4]}-{start_date[4:6]}-{start_date[6:8]}è‡³{end_date[:4]}-{end_date[4:6]}-{end_date[6:8]}"
        # è®¡ç®—å¤©æ•°å·®ä½œä¸ºæ•°æ®å‘¨æœŸ
        from datetime import datetime
        try:
            start = datetime.strptime(start_date, '%Y%m%d')
            end = datetime.strptime(end_date, '%Y%m%d')
            days_diff = (end - start).days + 1
            data_period = f"{days_diff}å¤©"
        except:
            data_period = "åŒºé—´å‘¨æœŸ"

    # 2. åŒ¹é…å•æ—¥æœŸæ ¼å¼ (YYYYMMDD)
    elif not date_range_match:
        single_date_pattern = r'(\d{8})'
        single_date_match = re.search(single_date_pattern, filename)
        if single_date_match:
            date_str = single_date_match.group(1)
            file_date = f"{date_str[:4]}-{date_str[4:6]}-{date_str[6:8]}"
            data_period = "å•æ—¥æ•°æ®"

    # 3. åŒ¹é…ç›¸å¯¹æ—¶é—´æ ¼å¼ (æ•°å­—+æ—¶é—´å•ä½)
    if file_date == 'æœªçŸ¥æ—¶é—´':
        relative_time_pattern = r'(\d+)([dmy])'
        relative_match = re.search(relative_time_pattern, filename.lower())
        if relative_match:
            number = relative_match.group(1)
            unit = relative_match.group(2)
            unit_map = {'d': 'å¤©', 'm': 'æœˆ', 'y': 'å¹´'}
            data_period = f"{number}{unit_map.get(unit, 'å¤©')}"
            file_date = f"ç›¸å¯¹æ—¶é—´({data_period})"

    # 4. ç‰¹æ®Šæ ¼å¼å…¼å®¹ (ä¿æŒå‘åå…¼å®¹)
    if file_date == 'æœªçŸ¥æ—¶é—´':
        if '30d' in filename.lower():
            file_date = 'ç›¸å¯¹æ—¶é—´(30å¤©)'
            data_period = '30å¤©'
        elif '7d' in filename.lower():
            file_date = 'ç›¸å¯¹æ—¶é—´(7å¤©)'
            data_period = '7å¤©'
        elif '1y' in filename.lower():
            file_date = 'ç›¸å¯¹æ—¶é—´(1å¹´)'
            data_period = '1å¹´'

    # æå–æ¦œå•ç±»å‹
    rank_types = ['é”€é‡æ¦œ', 'çƒ­æ¨æ¦œ', 'æ½œåŠ›æ¦œ', 'æŒç»­å¥½è´§æ¦œ', 'åŒæœŸæ¦œ']
    rank_type = 'æœªçŸ¥æ¦œå•'
    for rt in rank_types:
        if rt in filename:
            rank_type = rt
            break

    # æå–æ•°æ®æ¥æºè¡¨
    source_table = filename.split('-')[0] if '-' in filename else filename.split('.')[0]

    return {
        'snapshot_tag': file_date,  # ä¿æŒå‘åå…¼å®¹
        'rank_type': rank_type,
        'source_table': source_table,
        'file_date': file_date,
        'data_period': data_period
    }


def validate_file(uploaded_file) -> Tuple[bool, str]:
    """
    éªŒè¯ä¸Šä¼ çš„æ–‡ä»¶ï¼Œæ”¯æŒExcelå’ŒCSVæ ¼å¼

    å‚æ•°:
        uploaded_file: Streamlitä¸Šä¼ çš„æ–‡ä»¶å¯¹è±¡

    è¿”å›:
        (æ˜¯å¦æœ‰æ•ˆ, é”™è¯¯ä¿¡æ¯)
    """
    if uploaded_file is None:
        return False, "æ–‡ä»¶ä¸ºç©º"

    # æ£€æŸ¥æ–‡ä»¶æ‰©å±•å
    supported_extensions = ('.xlsx', '.xls', '.csv')
    if not uploaded_file.name.lower().endswith(supported_extensions):
        return False, f"ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {uploaded_file.name}ã€‚æ”¯æŒçš„æ ¼å¼: Excel(.xlsx, .xls) å’Œ CSV(.csv)"

    # æ£€æŸ¥æ–‡ä»¶å¤§å° (50MBé™åˆ¶)
    file_size_mb = uploaded_file.size / (1024 * 1024)
    if file_size_mb > 50:
        return False, f"æ–‡ä»¶è¿‡å¤§: {file_size_mb:.1f}MB (é™åˆ¶50MB)ã€‚å»ºè®®å‹ç¼©æ–‡ä»¶æˆ–åˆ†æ‰¹å¤„ç†"

    # å¯¹CSVæ–‡ä»¶è¿›è¡Œé¢å¤–çš„æ ¼å¼æ£€æŸ¥
    if uploaded_file.name.lower().endswith('.csv'):
        try:
            # å°è¯•è¯»å–CSVæ–‡ä»¶çš„å‰å‡ è¡Œè¿›è¡Œæ ¼å¼éªŒè¯
            uploaded_file.seek(0)
            test_df = smart_csv_reader(uploaded_file, nrows=1)
            if len(test_df.columns) == 0:
                return False, "CSVæ–‡ä»¶æ ¼å¼é”™è¯¯ï¼šæ— æ³•è¯†åˆ«åˆ—ç»“æ„ã€‚è¯·æ£€æŸ¥æ–‡ä»¶åˆ†éš”ç¬¦å’Œç¼–ç "
        except Exception as e:
            return False, f"CSVæ–‡ä»¶æ ¼å¼éªŒè¯å¤±è´¥ï¼š{str(e)}"
        finally:
            uploaded_file.seek(0)  # é‡ç½®æ–‡ä»¶æŒ‡é’ˆ

    # æ£€æŸ¥æ–‡ä»¶åæ—¶é—´æ ¼å¼å¹¶æä¾›å»ºè®®
    filename = uploaded_file.name
    metadata = extract_metadata_from_filename(filename)
    if metadata['file_date'] == 'æœªçŸ¥æ—¶é—´':
        suggestion = "å»ºè®®æ–‡ä»¶ååŒ…å«æ—¶é—´ä¿¡æ¯ï¼Œå¦‚: é”€é‡æ¦œ-20250622-20250630.xlsx æˆ– æ•°æ®-30d.csv"
        return True, f"æç¤º: æœªèƒ½ä»æ–‡ä»¶åæå–æ—¶é—´ä¿¡æ¯ã€‚{suggestion}"

    return True, ""


def process_single_file(uploaded_file, field_mapping: Dict[str, str]) -> Dict:
    """
    å¤„ç†å•ä¸ªæ•°æ®æ–‡ä»¶ï¼Œæ”¯æŒExcelå’ŒCSVæ ¼å¼

    å‚æ•°:
        uploaded_file: ä¸Šä¼ çš„æ–‡ä»¶
        field_mapping: å­—æ®µæ˜ å°„å­—å…¸

    è¿”å›:
        å¤„ç†ç»“æœå­—å…¸
    """
    try:
        # æ ¹æ®æ–‡ä»¶æ‰©å±•åé€‰æ‹©è¯»å–æ–¹å¼
        filename = uploaded_file.name.lower()
        if filename.endswith('.csv'):
            # ä½¿ç”¨æ™ºèƒ½CSVè¯»å–å™¨
            try:
                df = smart_csv_reader(uploaded_file)
            except Exception as e:
                # CSVæ–‡ä»¶è¯»å–çš„å…¶ä»–é”™è¯¯
                raise Exception(f"CSVæ–‡ä»¶è¯»å–å¤±è´¥: {str(e)}")
        else:
            # è¯»å–Excelæ–‡ä»¶
            try:
                df = pd.read_excel(uploaded_file, engine='openpyxl', dtype=str)
            except Exception as e:
                # Excelæ–‡ä»¶è¯»å–é”™è¯¯
                raise Exception(f"Excelæ–‡ä»¶è¯»å–å¤±è´¥: {str(e)}")
        
        if df.empty:
            return {
                'success': False,
                'filename': uploaded_file.name,
                'error': 'æ–‡ä»¶ä¸ºç©º',
                'data': None
            }
        
        # åº”ç”¨å­—æ®µæ˜ å°„
        df_mapped = df.rename(columns=field_mapping)

        # æ¸…ç†é‡å¤åˆ—å
        df_mapped = clean_duplicate_columns(df_mapped, "å­—æ®µæ˜ å°„å")

        # æå–æ–‡ä»¶åå…ƒæ•°æ®
        metadata = extract_metadata_from_filename(uploaded_file.name)

        # æ·»åŠ æ´¾ç”Ÿå­—æ®µï¼Œæ£€æŸ¥æ˜¯å¦ä¸ç°æœ‰åˆ—åå†²çª
        for field, value in metadata.items():
            if field not in df_mapped.columns:
                df_mapped[field] = value
            else:
                # å¦‚æœå­—æ®µå·²å­˜åœ¨ï¼Œæ›´æ–°å€¼ï¼ˆä¼˜å…ˆä½¿ç”¨å…ƒæ•°æ®ï¼‰
                df_mapped[field] = value
        
        # è°ƒç”¨ç”¨æˆ·çš„æ•°å€¼æ ‡å‡†åŒ–å‡½æ•°
        df_normalized = numeric_normalizer(df_mapped)

        # æ¸…ç†æ•°å€¼æ ‡å‡†åŒ–åçš„é‡å¤åˆ—å
        df_normalized = clean_duplicate_columns(df_normalized, "æ•°å€¼æ ‡å‡†åŒ–å")

        # ç¡®ä¿æ‰€æœ‰æ ‡å‡†å­—æ®µéƒ½å­˜åœ¨
        for std_field in STANDARD_FIELDS.keys():
            if std_field not in df_normalized.columns:
                df_normalized[std_field] = None

        # æ¸…ç†æ·»åŠ æ ‡å‡†å­—æ®µåçš„é‡å¤åˆ—å
        df_normalized = clean_duplicate_columns(df_normalized, "æ·»åŠ æ ‡å‡†å­—æ®µå")

        # åªä¿ç•™æ ‡å‡†å­—æ®µï¼Œç¡®ä¿åˆ—é¡ºåºä¸€è‡´
        available_fields = [field for field in STANDARD_FIELDS.keys() if field in df_normalized.columns]
        df_final = df_normalized[available_fields].copy()

        # æœ€ç»ˆæ¸…ç†ï¼šç¡®ä¿æ²¡æœ‰é‡å¤åˆ—å
        df_final = clean_duplicate_columns(df_final, "æœ€ç»ˆè¾“å‡º")
        
        return {
            'success': True,
            'filename': uploaded_file.name,
            'data': df_final,
            'original_rows': len(df),
            'processed_rows': len(df_final),
            'mapped_fields': len(field_mapping),
            'final_columns': len(df_final.columns),
            'has_duplicates': False  # ç»è¿‡å¤„ç†ååº”è¯¥æ²¡æœ‰é‡å¤åˆ—
        }
        
    except Exception as e:
        return {
            'success': False,
            'filename': uploaded_file.name,
            'error': f"å¤„ç†é”™è¯¯: {str(e)}",
            'data': None
        }


def create_download_zip(processed_results: List[Dict]) -> BytesIO:
    """
    åˆ›å»ºåŒ…å«æ‰€æœ‰å¤„ç†ç»“æœçš„ZIPæ–‡ä»¶
    
    å‚æ•°:
        processed_results: å¤„ç†ç»“æœåˆ—è¡¨
    
    è¿”å›:
        ZIPæ–‡ä»¶çš„BytesIOå¯¹è±¡
    """
    zip_buffer = BytesIO()
    
    with zipfile.ZipFile(zip_buffer, 'w', zipfile.ZIP_DEFLATED) as zip_file:
        for result in processed_results:
            if result['success'] and result['data'] is not None:
                # ç”Ÿæˆè¾“å‡ºæ–‡ä»¶åï¼Œæ”¯æŒCSVæ–‡ä»¶
                original_name = result['filename']
                clean_name = original_name.replace('.xlsx', '').replace('.xls', '').replace('.csv', '')
                output_filename = f"cleaned_{clean_name}.xlsx"

                # ç¡®ä¿æ•°æ®æ²¡æœ‰é‡å¤åˆ—å
                data_to_save = result['data'].copy()
                if data_to_save.columns.duplicated().any():
                    print(f"è­¦å‘Š: åœ¨ä¿å­˜ {original_name} æ—¶å‘ç°é‡å¤åˆ—åï¼Œæ­£åœ¨å»é‡")
                    data_to_save = data_to_save.loc[:, ~data_to_save.columns.duplicated()]

                # å°†DataFrameä¿å­˜åˆ°å†…å­˜ä¸­çš„Excelæ–‡ä»¶
                excel_buffer = BytesIO()
                data_to_save.to_excel(excel_buffer, index=False, engine='openpyxl')
                excel_buffer.seek(0)

                # æ·»åŠ åˆ°ZIPæ–‡ä»¶
                zip_file.writestr(output_filename, excel_buffer.getvalue())
    
    zip_buffer.seek(0)
    return zip_buffer


def render_sidebar():
    """æ¸²æŸ“ä¾§è¾¹æ æ§åˆ¶åŒº"""
    st.sidebar.title("ğŸ§¹ æ•°æ®æ¸…æ´—å·¥å…·")
    st.sidebar.markdown("---")
    
    # æ–‡ä»¶ä¸Šä¼ 
    st.sidebar.subheader("ğŸ“ ä¸Šä¼ æ–‡ä»¶")
    uploaded_files = st.sidebar.file_uploader(
        "é€‰æ‹©æ•°æ®æ–‡ä»¶",
        type=['xlsx', 'xls', 'csv'],
        accept_multiple_files=True,
        help="æ”¯æŒExcelæ–‡ä»¶(.xlsx, .xls)å’ŒCSVæ–‡ä»¶(.csv)çš„æ‰¹é‡ä¸Šä¼ "
    )
    
    # å­—æ®µæ˜ å°„é¢„è§ˆ
    if uploaded_files:
        st.sidebar.subheader("ğŸ”— å­—æ®µæ˜ å°„é¢„è§ˆ")
        
        # åˆ†æç¬¬ä¸€ä¸ªæ–‡ä»¶çš„å­—æ®µ
        first_file = uploaded_files[0]
        try:
            # æ ¹æ®æ–‡ä»¶ç±»å‹è¯»å–è¡¨å¤´
            filename = first_file.name.lower()
            if filename.endswith('.csv'):
                # ä½¿ç”¨æ™ºèƒ½CSVè¯»å–å™¨è¯»å–è¡¨å¤´
                sample_df = smart_csv_reader(first_file, nrows=0)
            else:
                # è¯»å–Excelæ–‡ä»¶è¡¨å¤´
                sample_df = pd.read_excel(first_file, nrows=0)

            auto_mapping = smart_field_mapping(sample_df.columns.tolist())

            # æ˜¾ç¤ºæ—¶é—´ä¿¡æ¯æå–é¢„è§ˆ
            metadata = extract_metadata_from_filename(first_file.name)

            if auto_mapping:
                st.sidebar.success(f"âœ… è‡ªåŠ¨è¯†åˆ«åˆ° {len(auto_mapping)} ä¸ªå­—æ®µ")
                with st.sidebar.expander("æŸ¥çœ‹æ˜ å°„è¯¦æƒ…"):
                    for raw, std in auto_mapping.items():
                        st.write(f"**{raw}** â†’ {STANDARD_FIELDS[std]}")

                    # æ˜¾ç¤ºæ—¶é—´ä¿¡æ¯æå–ç»“æœ
                    st.write("---")
                    st.write("**ğŸ“… æ—¶é—´ä¿¡æ¯æå–:**")
                    st.write(f"â€¢ **æ–‡ä»¶æ—¥æœŸ**: {metadata['file_date']}")
                    st.write(f"â€¢ **æ•°æ®å‘¨æœŸ**: {metadata['data_period']}")
                    st.write(f"â€¢ **æ¦œå•ç±»å‹**: {metadata['rank_type']}")
            else:
                st.sidebar.warning("âš ï¸ æœªèƒ½è‡ªåŠ¨è¯†åˆ«å­—æ®µï¼Œè¯·æ£€æŸ¥æ–‡ä»¶æ ¼å¼")

        except Exception as e:
            st.sidebar.error(f"âŒ æ–‡ä»¶é¢„è§ˆå¤±è´¥: {str(e)}")
    
    return uploaded_files


def render_main_content(uploaded_files):
    """æ¸²æŸ“ä¸»å†…å®¹åŒº"""
    st.title("ğŸ§¹ æŠ–éŸ³ç”µå•†æ•°æ®æ¸…æ´—å·¥å…·")
    st.markdown("**ä¸“ä¸šçš„Excelæ•°æ®æ ‡å‡†åŒ–å¤„ç†å¹³å°**")
    
    if not uploaded_files:
        # æ˜¾ç¤ºä½¿ç”¨è¯´æ˜
        st.info("ğŸ‘ˆ è¯·åœ¨å·¦ä¾§ä¸Šä¼ Excelæ–‡ä»¶å¼€å§‹æ•°æ®æ¸…æ´—")
        
        col1, col2 = st.columns(2)
        
        with col1:
            st.subheader("ğŸ“‹ æ”¯æŒçš„æ ‡å‡†å­—æ®µ")
            for i, (field, desc) in enumerate(STANDARD_FIELDS.items()):
                if i < len(STANDARD_FIELDS) // 2:
                    st.write(f"â€¢ **{desc}** (`{field}`)")
        
        with col2:
            st.subheader("ğŸ“‹ æ”¯æŒçš„æ ‡å‡†å­—æ®µï¼ˆç»­ï¼‰")
            for i, (field, desc) in enumerate(STANDARD_FIELDS.items()):
                if i >= len(STANDARD_FIELDS) // 2:
                    st.write(f"â€¢ **{desc}** (`{field}`)")
        
        st.subheader("ğŸš€ åŠŸèƒ½ç‰¹ç‚¹")
        st.write("""
        - âœ… **å¤šæ ¼å¼æ”¯æŒ**ï¼šæ”¯æŒExcel(.xlsx, .xls)å’ŒCSV(.csv)æ–‡ä»¶
        - âœ… **æ™ºèƒ½å­—æ®µæ˜ å°„**ï¼šè‡ªåŠ¨è¯†åˆ«å’ŒåŒ¹é…19ä¸ªæ ‡å‡†å­—æ®µ
        - âœ… **æ™ºèƒ½æ—¶é—´æå–**ï¼šæ”¯æŒå¤šç§æ–‡ä»¶åæ—¶é—´æ ¼å¼è¯†åˆ«
        - âœ… **æ‰¹é‡å¤„ç†**ï¼šæ”¯æŒåŒæ—¶å¤„ç†å¤šä¸ªæ•°æ®æ–‡ä»¶
        - âœ… **æ•°å€¼æ ‡å‡†åŒ–**ï¼šå¤„ç†ç™¾åˆ†æ¯”ã€åŒºé—´å€¼ã€å•ä½è½¬æ¢
        - âœ… **å…ƒæ•°æ®æå–**ï¼šä»æ–‡ä»¶åè‡ªåŠ¨æå–æ¦œå•ç±»å‹å’Œæ—¶é—´ä¿¡æ¯
        - âœ… **ZIPæ‰“åŒ…ä¸‹è½½**ï¼šä¸€é”®ä¸‹è½½æ‰€æœ‰å¤„ç†ç»“æœ
        - âœ… **å‹å¥½æç¤º**ï¼šè¯¦ç»†çš„å¤„ç†è¿›åº¦å’Œé”™è¯¯ä¿¡æ¯
        """)
        
        return
    
    # æ˜¾ç¤ºæ–‡ä»¶ä¿¡æ¯
    st.subheader(f"ğŸ“ å·²ä¸Šä¼  {len(uploaded_files)} ä¸ªæ–‡ä»¶")
    
    # æ–‡ä»¶åˆ—è¡¨
    file_info = []
    total_size = 0
    
    for file in uploaded_files:
        is_valid, error_msg = validate_file(file)
        file_size_mb = file.size / (1024 * 1024)
        total_size += file_size_mb
        
        file_info.append({
            'æ–‡ä»¶å': file.name,
            'å¤§å°(MB)': f"{file_size_mb:.2f}",
            'çŠ¶æ€': "âœ… æœ‰æ•ˆ" if is_valid else f"âŒ {error_msg}"
        })
    
    # æ˜¾ç¤ºæ–‡ä»¶ä¿¡æ¯è¡¨æ ¼
    df_files = pd.DataFrame(file_info)
    st.dataframe(df_files, use_container_width=True)
    
    # å¤§æ–‡ä»¶è­¦å‘Š
    if total_size > 50:
        st.warning(f"âš ï¸ æ€»æ–‡ä»¶å¤§å° {total_size:.1f}MB è¾ƒå¤§ï¼Œå¤„ç†å¯èƒ½éœ€è¦æ›´å¤šæ—¶é—´")
    
    # å¼€å§‹å¤„ç†æŒ‰é’®
    if st.button("ğŸš€ å¼€å§‹æ¸…æ´—", type="primary", use_container_width=True):
        process_files(uploaded_files)


def process_files(uploaded_files):
    """å¤„ç†æ–‡ä»¶çš„ä¸»è¦é€»è¾‘"""
    # åˆå§‹åŒ–è¿›åº¦æ˜¾ç¤º
    progress_bar = st.progress(0)
    status_text = st.empty()
    
    # å­˜å‚¨å¤„ç†ç»“æœ
    if 'processing_results' not in st.session_state:
        st.session_state.processing_results = []
    
    st.session_state.processing_results = []
    
    # å¤„ç†æ¯ä¸ªæ–‡ä»¶
    for i, uploaded_file in enumerate(uploaded_files):
        status_text.text(f"æ­£åœ¨å¤„ç†æ–‡ä»¶ {i+1}/{len(uploaded_files)}: {uploaded_file.name}")
        
        # éªŒè¯æ–‡ä»¶
        is_valid, error_msg = validate_file(uploaded_file)
        if not is_valid:
            st.session_state.processing_results.append({
                'success': False,
                'filename': uploaded_file.name,
                'error': error_msg,
                'data': None
            })
            continue
        
        # è·å–å­—æ®µæ˜ å°„
        try:
            # æ ¹æ®æ–‡ä»¶ç±»å‹é€‰æ‹©æ­£ç¡®çš„è¯»å–æ–¹å¼
            filename = uploaded_file.name.lower()
            if filename.endswith('.csv'):
                # ä½¿ç”¨æ™ºèƒ½CSVè¯»å–å™¨è¯»å–è¡¨å¤´
                sample_df = smart_csv_reader(uploaded_file, nrows=0)
            else:
                # è¯»å–Excelæ–‡ä»¶è¡¨å¤´
                sample_df = pd.read_excel(uploaded_file, nrows=0)

            field_mapping = smart_field_mapping(sample_df.columns.tolist())
        except Exception as e:
            # æä¾›æ›´è¯¦ç»†çš„é”™è¯¯ä¿¡æ¯ï¼ŒåŒºåˆ†æ–‡ä»¶ç±»å‹
            file_type = "CSV" if uploaded_file.name.lower().endswith('.csv') else "Excel"
            st.session_state.processing_results.append({
                'success': False,
                'filename': uploaded_file.name,
                'error': f"è¯»å–{file_type}æ–‡ä»¶å¤±è´¥: {str(e)}",
                'data': None
            })
            continue
        
        # å¤„ç†æ–‡ä»¶
        result = process_single_file(uploaded_file, field_mapping)
        st.session_state.processing_results.append(result)
        
        # æ›´æ–°è¿›åº¦
        progress_bar.progress((i + 1) / len(uploaded_files))
    
    # å¤„ç†å®Œæˆ
    status_text.text("âœ… å¤„ç†å®Œæˆï¼")
    
    # æ˜¾ç¤ºå¤„ç†ç»“æœ
    show_processing_results()


def show_processing_results():
    """æ˜¾ç¤ºå¤„ç†ç»“æœ"""
    if 'processing_results' not in st.session_state or not st.session_state.processing_results:
        return
    
    results = st.session_state.processing_results
    
    # ç»Ÿè®¡ä¿¡æ¯
    successful_count = sum(1 for r in results if r['success'])
    failed_count = len(results) - successful_count

    # è®¡ç®—æ—¶é—´ä¿¡æ¯æå–æˆåŠŸç‡
    time_extraction_success = 0
    for r in results:
        if r['success'] and r.get('data') is not None:
            # æ£€æŸ¥æ˜¯å¦æˆåŠŸæå–æ—¶é—´ä¿¡æ¯
            if 'file_date' in r['data'].columns:
                time_info = r['data']['file_date'].iloc[0] if len(r['data']) > 0 else 'æœªçŸ¥æ—¶é—´'
                if time_info != 'æœªçŸ¥æ—¶é—´':
                    time_extraction_success += 1

    time_extraction_rate = (time_extraction_success / successful_count * 100) if successful_count > 0 else 0

    st.subheader("ğŸ“Š å¤„ç†ç»“æœç»Ÿè®¡")

    col1, col2, col3, col4 = st.columns(4)
    with col1:
        st.metric("æ€»æ–‡ä»¶æ•°", len(results))
    with col2:
        st.metric("æˆåŠŸå¤„ç†", successful_count, delta=None)
    with col3:
        st.metric("å¤„ç†å¤±è´¥", failed_count, delta=None)
    with col4:
        st.metric("æ—¶é—´æå–æˆåŠŸç‡", f"{time_extraction_rate:.1f}%", delta=None)
    
    # è¯¦ç»†ç»“æœ
    if successful_count > 0:
        st.subheader("âœ… å¤„ç†æˆåŠŸçš„æ–‡ä»¶")
        
        # æ˜¾ç¤ºç¬¬ä¸€ä¸ªæˆåŠŸæ–‡ä»¶çš„é¢„è§ˆ
        first_success = next(r for r in results if r['success'])
        if first_success['data'] is not None:
            st.write("**æ•°æ®é¢„è§ˆï¼ˆå‰10è¡Œï¼‰ï¼š**")

            # åˆ›å»ºé¢„è§ˆæ•°æ®ï¼Œçªå‡ºæ˜¾ç¤ºæ—¶é—´å­—æ®µ
            preview_data = first_success['data'].head(10).copy()

            # æœ€ç»ˆæ£€æŸ¥ï¼šç¡®ä¿é¢„è§ˆæ•°æ®æ²¡æœ‰é‡å¤åˆ—å
            if preview_data.columns.duplicated().any():
                st.warning("âš ï¸ æ£€æµ‹åˆ°é‡å¤åˆ—åï¼Œæ­£åœ¨è‡ªåŠ¨ä¿®å¤...")
                duplicated_cols = preview_data.columns[preview_data.columns.duplicated()].tolist()
                st.write(f"é‡å¤çš„åˆ—å: {duplicated_cols}")
                preview_data = clean_duplicate_columns(preview_data, "æ•°æ®é¢„è§ˆ")
                st.success("âœ… é‡å¤åˆ—åå·²è‡ªåŠ¨å»é™¤")

            # å¦‚æœå­˜åœ¨æ—¶é—´å­—æ®µï¼Œåœ¨åˆ—åå‰æ·»åŠ ç‰¹æ®Šæ ‡è®°
            if 'file_date' in preview_data.columns and 'data_period' in preview_data.columns:
                st.info("ğŸ’¡ **æ—¶é—´å­—æ®µè¯´æ˜**: ğŸ“… file_date(æ–‡ä»¶æ—¥æœŸ) å’Œ â±ï¸ data_period(æ•°æ®å‘¨æœŸ) ä¸ºè‡ªåŠ¨æå–çš„æ—¶é—´ä¿¡æ¯")

            # æ˜¾ç¤ºåˆ—ä¿¡æ¯
            st.write(f"**åˆ—ä¿¡æ¯**: å…± {len(preview_data.columns)} åˆ—")

            try:
                st.dataframe(preview_data, use_container_width=True)
            except Exception as e:
                st.error(f"æ•°æ®é¢„è§ˆæ˜¾ç¤ºå¤±è´¥: {str(e)}")
                st.write("**åˆ—ååˆ—è¡¨:**")
                st.write(list(preview_data.columns))

            # æ˜¾ç¤ºæ—¶é—´æå–è¯¦æƒ…
            if len(preview_data) > 0:
                st.write("**ğŸ“… æ—¶é—´ä¿¡æ¯æå–è¯¦æƒ…ï¼š**")
                col1, col2 = st.columns(2)
                with col1:
                    file_date_value = preview_data['file_date'].iloc[0] if 'file_date' in preview_data.columns else 'æœªæå–'
                    st.write(f"â€¢ **æ–‡ä»¶æ—¥æœŸ**: {file_date_value}")
                with col2:
                    data_period_value = preview_data['data_period'].iloc[0] if 'data_period' in preview_data.columns else 'æœªæå–'
                    st.write(f"â€¢ **æ•°æ®å‘¨æœŸ**: {data_period_value}")
        
        # åˆ›å»ºä¸‹è½½é“¾æ¥
        zip_data = create_download_zip([r for r in results if r['success']])
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"cleaned_data_{timestamp}.zip"
        
        st.download_button(
            label="ğŸ“¥ ä¸‹è½½æ‰€æœ‰ç»“æœ (ZIP)",
            data=zip_data,
            file_name=filename,
            mime="application/zip",
            type="primary",
            use_container_width=True
        )
    
    # å¤±è´¥æ–‡ä»¶åˆ—è¡¨
    if failed_count > 0:
        st.subheader("âŒ å¤„ç†å¤±è´¥çš„æ–‡ä»¶")
        failed_files = [r for r in results if not r['success']]
        
        for failed in failed_files:
            st.error(f"**{failed['filename']}**: {failed['error']}")


def main():
    """ä¸»åº”ç”¨å…¥å£"""
    # æ¸²æŸ“ä¾§è¾¹æ å¹¶è·å–ä¸Šä¼ çš„æ–‡ä»¶
    uploaded_files = render_sidebar()
    
    # æ¸²æŸ“ä¸»å†…å®¹åŒº
    render_main_content(uploaded_files)
    
    # é¡µè„šä¿¡æ¯
    st.markdown("---")
    st.markdown(
        "<div style='text-align: center; color: #666;'>"
        "ğŸ§¹ æŠ–éŸ³ç”µå•†æ•°æ®æ¸…æ´—å·¥å…· | ä¸“ä¸šæ•°æ®å¤„ç†è§£å†³æ–¹æ¡ˆ"
        "</div>",
        unsafe_allow_html=True
    )


if __name__ == "__main__":
    main()
