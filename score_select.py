#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç”µå•†æ•°æ®æ™ºèƒ½è¿‡æ»¤è¯„åˆ†ç³»ç»Ÿ
åŠŸèƒ½ï¼šå¤šç»´åº¦è¯„åˆ†ç®—æ³• + èŠ‚æ—¥æ¨¡å¼æ„ŸçŸ¥ + TOP50ç­›é€‰
ä½œè€…ï¼šæ•°æ®è¿‡æ»¤ä¸“å®¶
"""

import pandas as pd
import numpy as np
import argparse
import os
import glob
import re
from datetime import datetime, timedelta
import warnings
warnings.filterwarnings('ignore')

def get_base_weights():
    """è·å–åŸºç¡€æƒé‡é…ç½®ï¼ˆæ”¯æŒåŠ¨æ€è°ƒæ•´ï¼‰"""
    return {
        'sales_30d_score': 0.12,
        'gmv_30d_score': 0.08,
        'sales_7d_score': 0.08,
        'gmv_7d_score': 0.07,
        'commission_score': 0.15,
        'influencer_score': 0.10,
        'rank_score': 0.12,
        'growth_score': 0.08,
        'channel_score': 0.05,
        'conv_score': 0.05,
        'live_gmv_score': 0.05,
        'card_gmv_score': 0.05
    }

def parse_file_date(date_string):
    """
    è§£æfile_dateå­—æ®µï¼Œæ”¯æŒå¤šç§æ ¼å¼ï¼š
    - å•ä¸€æ—¥æœŸ: YYYY-MM-DD â†’ ç›´æ¥è¿”å›
    - æ—¥æœŸèŒƒå›´: YYYY-MM-DDè‡³YYYY-MM-DD â†’ è¿”å›ä¸­ç‚¹æ—¥æœŸ
    - æ— æ•ˆæ ¼å¼ â†’ è¿”å›å½“å‰æ—¥æœŸä½œä¸ºå¤‡ç”¨

    Args:
        date_string: file_dateåˆ—çš„å­—ç¬¦ä¸²å€¼

    Returns:
        str: è§£æåçš„æ ‡å‡†æ—¥æœŸæ ¼å¼ YYYY-MM-DD
    """
    if pd.isna(date_string):
        return pd.Timestamp.now().strftime('%Y-%m-%d')

    date_string = str(date_string).strip()

    # å¤„ç†æ—¥æœŸèŒƒå›´æ ¼å¼: YYYY-MM-DDè‡³YYYY-MM-DD
    range_match = re.match(r'(\d{4}-\d{2}-\d{2})è‡³(\d{4}-\d{2}-\d{2})', date_string)
    if range_match:
        start_date, end_date = range_match.groups()
        try:
            d1 = pd.to_datetime(start_date)
            d2 = pd.to_datetime(end_date)
            midpoint = d1 + (d2 - d1) / 2
            return midpoint.strftime('%Y-%m-%d')
        except:
            return pd.Timestamp.now().strftime('%Y-%m-%d')

    # å¤„ç†å•ä¸€æ—¥æœŸæ ¼å¼: YYYY-MM-DD
    try:
        parsed_date = pd.to_datetime(date_string, format="%Y-%m-%d")
        return parsed_date.strftime('%Y-%m-%d')
    except:
        # å°è¯•å…¶ä»–å¸¸è§æ ¼å¼
        try:
            parsed_date = pd.to_datetime(date_string)
            return parsed_date.strftime('%Y-%m-%d')
        except:
            return pd.Timestamp.now().strftime('%Y-%m-%d')

def calculate_days_to_next_holiday(file_date):
    """è®¡ç®—è·ç¦»ä¸‹ä¸€ä¸ªèŠ‚æ—¥çš„å¤©æ•°ï¼ˆæ”¯æŒè§£æåçš„æ ‡å‡†æ—¥æœŸæ ¼å¼ï¼‰"""
    holidays = [
        (1, 1),   # å…ƒæ—¦
        (2, 14),  # æƒ…äººèŠ‚
        (3, 8),   # å¦‡å¥³èŠ‚
        (6, 1),   # å„¿ç«¥èŠ‚
        (9, 15),  # ä¸­ç§‹
        (10, 1),  # å›½åº†
        (12, 25)  # åœ£è¯
    ]

    try:
        # file_dateç°åœ¨ä¿è¯æ˜¯YYYY-MM-DDæ ¼å¼
        current_date = pd.to_datetime(file_date)
        current_year = current_date.year
        min_days = float('inf')

        for month, day in holidays:
            # å½“å¹´èŠ‚æ—¥
            holiday_date = pd.to_datetime(f"{current_year}-{month:02d}-{day:02d}")
            if holiday_date >= current_date:
                days = (holiday_date - current_date).days
                min_days = min(min_days, days)

            # ä¸‹ä¸€å¹´èŠ‚æ—¥
            next_year_holiday = pd.to_datetime(f"{current_year+1}-{month:02d}-{day:02d}")
            days = (next_year_holiday - current_date).days
            min_days = min(min_days, days)

        return min_days
    except Exception as e:
        print(f"âš ï¸ èŠ‚æ—¥è·ç¦»è®¡ç®—å¤±è´¥: {e}ï¼Œä½¿ç”¨é»˜è®¤å€¼999å¤©")
        return 999

def adjust_weights_for_available_fields(df, base_weights):
    """
    æ ¹æ®æ•°æ®æ–‡ä»¶ä¸­å®é™…å­˜åœ¨çš„å­—æ®µåŠ¨æ€è°ƒæ•´æƒé‡

    Args:
        df: è¾“å…¥æ•°æ®DataFrame
        base_weights: åŸºç¡€æƒé‡å­—å…¸

    Returns:
        dict: è°ƒæ•´åçš„æƒé‡å­—å…¸

    Raises:
        ValueError: å½“ç¼ºå°‘æ‰€æœ‰é”€é‡/GMVå­—æ®µæ—¶
    """
    weights = base_weights.copy()

    # æ£€æŸ¥å­—æ®µå­˜åœ¨æ€§
    has_7d_fields = all(col in df.columns for col in ['sales_7d', 'gmv_7d'])
    has_30d_fields = all(col in df.columns for col in ['sales_30d', 'gmv_30d'])

    print(f"ğŸ” å­—æ®µæ£€æŸ¥: 7å¤©å­—æ®µ={has_7d_fields}, 30å¤©å­—æ®µ={has_30d_fields}")

    if has_7d_fields and has_30d_fields:
        # åœºæ™¯A: å®Œæ•´æ•°æ®ï¼Œä½¿ç”¨é»˜è®¤æƒé‡
        print("âœ… åœºæ™¯A: å®Œæ•´æ•°æ®ï¼Œä½¿ç”¨é»˜è®¤æƒé‡é…ç½®")
        return weights

    elif has_30d_fields and not has_7d_fields:
        # åœºæ™¯B: ä»…æœ‰30å¤©æ•°æ®ï¼Œå°†7å¤©æƒé‡è½¬ç§»ç»™30å¤©
        print("âš ï¸ åœºæ™¯B: ä»…æœ‰30å¤©æ•°æ®ï¼Œå°†7å¤©æƒé‡è½¬ç§»ç»™30å¤©å­—æ®µ")
        boost_total = weights['sales_7d_score'] + weights['gmv_7d_score']
        current_30d_total = weights['sales_30d_score'] + weights['gmv_30d_score']

        # æŒ‰æ¯”ä¾‹åˆ†é…å¢åŠ çš„æƒé‡
        sales_boost = boost_total * (weights['sales_30d_score'] / current_30d_total)
        gmv_boost = boost_total * (weights['gmv_30d_score'] / current_30d_total)

        weights['sales_30d_score'] += sales_boost
        weights['gmv_30d_score'] += gmv_boost
        weights['sales_7d_score'] = 0
        weights['gmv_7d_score'] = 0

        print(f"  ğŸ“Š æƒé‡è°ƒæ•´: sales_30d {weights['sales_30d_score']-sales_boost:.3f}â†’{weights['sales_30d_score']:.3f}")
        print(f"  ğŸ“Š æƒé‡è°ƒæ•´: gmv_30d {weights['gmv_30d_score']-gmv_boost:.3f}â†’{weights['gmv_30d_score']:.3f}")

    elif has_7d_fields and not has_30d_fields:
        # åœºæ™¯C: ä»…æœ‰7å¤©æ•°æ®ï¼Œå°†30å¤©æƒé‡è½¬ç§»ç»™7å¤©
        print("âš ï¸ åœºæ™¯C: ä»…æœ‰7å¤©æ•°æ®ï¼Œå°†30å¤©æƒé‡è½¬ç§»ç»™7å¤©å­—æ®µ")
        boost_total = weights['sales_30d_score'] + weights['gmv_30d_score']
        current_7d_total = weights['sales_7d_score'] + weights['gmv_7d_score']

        # æŒ‰æ¯”ä¾‹åˆ†é…å¢åŠ çš„æƒé‡
        sales_boost = boost_total * (weights['sales_7d_score'] / current_7d_total)
        gmv_boost = boost_total * (weights['gmv_7d_score'] / current_7d_total)

        weights['sales_7d_score'] += sales_boost
        weights['gmv_7d_score'] += gmv_boost
        weights['sales_30d_score'] = 0
        weights['gmv_30d_score'] = 0

        print(f"  ğŸ“Š æƒé‡è°ƒæ•´: sales_7d {weights['sales_7d_score']-sales_boost:.3f}â†’{weights['sales_7d_score']:.3f}")
        print(f"  ğŸ“Š æƒé‡è°ƒæ•´: gmv_7d {weights['gmv_7d_score']-gmv_boost:.3f}â†’{weights['gmv_7d_score']:.3f}")

    else:
        # åœºæ™¯D: ç¼ºå°‘æ‰€æœ‰é”€é‡/GMVå­—æ®µ
        raise ValueError("æ•°æ®æ–‡ä»¶ç¼ºå°‘å¿…è¦çš„é”€é‡å’ŒGMVå­—æ®µï¼ˆsales_7d/30d, gmv_7d/30dï¼‰ï¼Œæ— æ³•è¿›è¡Œè¯„åˆ†")

    # éªŒè¯æƒé‡æ€»å’Œ
    total_weight = sum(weights.values())
    print(f"âœ… æƒé‡æ€»å’ŒéªŒè¯: {total_weight:.6f}")

    return weights

def adjust_holiday_weights(base_weights, is_holiday_mode):
    """èŠ‚æ—¥æ¨¡å¼æƒé‡åŠ¨æ€è°ƒæ•´"""
    if not is_holiday_mode:
        return base_weights

    adjusted = base_weights.copy()

    # ä¼˜å…ˆè°ƒæ•´å­˜åœ¨çš„7å¤©é”€é‡æƒé‡ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™è°ƒæ•´30å¤©æƒé‡
    if adjusted['sales_7d_score'] > 0:
        adjusted['sales_7d_score'] += 0.02
        boost_field = 'sales_7d_score'
    elif adjusted['sales_30d_score'] > 0:
        adjusted['sales_30d_score'] += 0.02
        boost_field = 'sales_30d_score'
    else:
        # å¦‚æœéƒ½æ²¡æœ‰é”€é‡å­—æ®µï¼Œåˆ™ä¸è¿›è¡ŒèŠ‚æ—¥è°ƒæ•´
        return adjusted

    # å…¶ä»–æƒé‡æŒ‰æ¯”ä¾‹ç¼©æ”¾
    other_keys = [k for k in adjusted.keys() if k != boost_field]
    total_others = sum(adjusted[k] for k in other_keys)
    scale_factor = (1 - adjusted[boost_field]) / total_others

    for key in other_keys:
        adjusted[key] *= scale_factor

    print(f"ğŸ„ èŠ‚æ—¥æ¨¡å¼: {boost_field} æƒé‡å¢åŠ 0.02")

    return adjusted

def clip_and_normalize(series, percentile=99):
    """é•¿å°¾æˆªæ–­ä¸æ ‡å‡†åŒ–å¤„ç†"""
    if series.isna().all():
        return pd.Series(0, index=series.index)
    
    # 99åˆ†ä½æˆªæ–­
    upper_bound = series.quantile(percentile / 100)
    clipped = series.clip(upper=upper_bound)
    
    # å¯¹æ•°å˜æ¢
    log_transformed = np.log(clipped + 1)
    
    # Min-Maxæ ‡å‡†åŒ–
    min_val = log_transformed.min()
    max_val = log_transformed.max()
    if max_val == min_val:
        return pd.Series(0, index=series.index)
    
    normalized = (log_transformed - min_val) / (max_val - min_val)
    return normalized

def score_commission(commission_series):
    """ä½£é‡‘åˆ†æ®µè¯„åˆ†ç®—æ³•"""
    def calc_score(c):
        if pd.isna(c):
            return 0
        if c < 0.25:
            return c / 0.25
        elif 0.25 <= c < 0.3:
            return 1.0 + 0.1
        elif 0.3 <= c < 0.35:
            return 1.0 + 0.15
        else:
            return 1.0 + 0.2
    
    return commission_series.apply(calc_score)

def cosine_decay_score(influencer_series):
    """è¾¾äººå½±å“åŠ›ä½™å¼¦è¡°å‡è¯„åˆ†"""
    if influencer_series.isna().all() or influencer_series.mean() == 0:
        return pd.Series(0, index=influencer_series.index)
    
    n = influencer_series.fillna(0)
    mean_n = n.mean()
    mean_n_squared = mean_n ** 2
    
    score = n / np.sqrt(n**2 + mean_n_squared)
    return score

def rank_score_with_decay(rank_type_series, rank_no_series, is_holiday_mode=False):
    """æ’åæŒ‡æ•°è¡°å‡ + ç±»å‹åŸºç¡€åˆ†"""
    # ç±»å‹åŸºç¡€åˆ†
    base_scores = rank_type_series.map({
        'æ½œåŠ›æ¦œ': 0.4,
        'é”€é‡æ¦œ': 0.3,
        'å…¶ä»–': 0.2
    }).fillna(0.2)

    # èŠ‚æ—¥æ¨¡å¼ä¸‹é”€é‡æ¦œåŸºç¡€åˆ†+0.02
    if is_holiday_mode:
        base_scores = base_scores.where(rank_type_series != 'é”€é‡æ¦œ', base_scores + 0.02)

    # æŒ‡æ•°è¡°å‡éƒ¨åˆ†
    rank_no_filled = rank_no_series.fillna(999)
    rank_part = np.exp(-0.015 * (rank_no_filled - 1))

    # ç»„åˆå¾—åˆ†
    final_score = base_scores * 0.4 + rank_part * 0.6
    return final_score

def growth_potential_score(sales_30d, sales_1y):
    """å¢é•¿æ½œåŠ›è¯„åˆ†ï¼šé”€å”®1y Ã— å¢é•¿ç‡"""
    # è®¡ç®—æœˆå‡é”€é‡
    monthly_avg = sales_1y / 12

    # è®¡ç®—å¢é•¿ç‡
    growth_rate = sales_30d / (monthly_avg + 1)

    # å¤§å•†å®¶æƒ©ç½š
    penalty = np.where(sales_1y > 5e4, 0.2, 0)

    # æœ€ç»ˆå¾—åˆ†
    score = np.clip(growth_rate - penalty, 0, 1)
    return score

def channel_distribution_score(live_gmv_30d, live_gmv_7d, card_gmv_30d, gmv_30d, gmv_7d):
    """æ¸ é“åˆ†å¸ƒè¯„åˆ†ç®—æ³•ï¼ˆæ”¯æŒç¼ºå¤±å­—æ®µï¼‰"""
    # å®‰å…¨åœ°è®¡ç®—å„æ¸ é“å æ¯”ï¼Œå¤„ç†NaNå’Œç¼ºå¤±å€¼
    live_ratio_30d = np.where(
        (gmv_30d > 0) & (~pd.isna(live_gmv_30d)) & (~pd.isna(gmv_30d)),
        live_gmv_30d / gmv_30d,
        0.3  # é»˜è®¤ç›´æ’­å æ¯”30%
    )

    live_ratio_7d = np.where(
        (gmv_7d > 0) & (~pd.isna(live_gmv_7d)) & (~pd.isna(gmv_7d)),
        live_gmv_7d / gmv_7d,
        0.3  # é»˜è®¤ç›´æ’­å æ¯”30%
    )

    card_ratio_30d = np.where(
        (gmv_30d > 0) & (~pd.isna(card_gmv_30d)) & (~pd.isna(gmv_30d)),
        card_gmv_30d / gmv_30d,
        0.2  # é»˜è®¤å•†å“å¡å æ¯”20%
    )

    # æ¸ é“å¾—åˆ†è®¡ç®—
    channel_score = (
        (1 - live_ratio_30d) * 0.03 +  # é™ä½ç›´æ’­ä¾èµ–
        (1 - live_ratio_7d) * 0.02 +   # çŸ­æœŸç›´æ’­ä¾èµ–
        card_ratio_30d * 0.05           # å•†å“å¡è¡¨ç°
    )

    # ç¡®ä¿è¿”å›å€¼åœ¨åˆç†èŒƒå›´å†…
    channel_score = np.clip(channel_score, 0, 1)

    return channel_score

def filter_and_score_conversion(conv_30d_series):
    """è½¬åŒ–ç‡é˜ˆå€¼è¿‡æ»¤ä¸è¯„åˆ†"""
    # é˜ˆå€¼è¿‡æ»¤
    valid_mask = conv_30d_series >= 0.02

    # çº¿æ€§æ˜ å°„ [0, 0.2] â†’ [0, 1]
    clipped = conv_30d_series.clip(0, 0.2)
    normalized = clipped / 0.2

    # åº”ç”¨æƒé‡
    score = normalized * 0.08

    return score, valid_mask

def read_data_file(file_path):
    """è¯»å–CSV/XLSXæ–‡ä»¶"""
    try:
        if file_path.endswith('.xlsx'):
            df = pd.read_excel(file_path)
        else:
            # å°è¯•ä¸åŒç¼–ç 
            for encoding in ['utf-8', 'gbk', 'gb2312']:
                try:
                    df = pd.read_csv(file_path, encoding=encoding)
                    break
                except:
                    continue
            else:
                df = pd.read_csv(file_path)

        return df
    except Exception as e:
        print(f"è¯»å–æ–‡ä»¶å¤±è´¥ {file_path}: {e}")
        return None

def process_single_file(df, file_date, is_holiday_mode):
    """å¤„ç†å•ä¸ªæ–‡ä»¶çš„è¯„åˆ†è®¡ç®—ï¼ˆæ”¯æŒåŠ¨æ€æƒé‡å’Œæ—¥æœŸè§£æï¼‰"""
    print(f"\nğŸ”„ å¼€å§‹å¤„ç†æ–‡ä»¶æ•°æ®...")

    # è§£æå’Œæ ‡å‡†åŒ–æ–‡ä»¶æ—¥æœŸ
    if 'file_date' in df.columns and len(df) > 0:
        raw_file_date = df['file_date'].iloc[0]
        parsed_file_date = parse_file_date(raw_file_date)
        print(f"ğŸ“… æ—¥æœŸè§£æ: {raw_file_date} â†’ {parsed_file_date}")
        file_date = parsed_file_date
    else:
        file_date = datetime.now().strftime('%Y-%m-%d')
        print(f"ğŸ“… ä½¿ç”¨é»˜è®¤æ—¥æœŸ: {file_date}")

    # æ£€æµ‹èŠ‚æ—¥æ¨¡å¼
    days_to_holiday = calculate_days_to_next_holiday(file_date)
    is_holiday_mode = days_to_holiday <= 45

    if is_holiday_mode:
        print(f"ğŸ„ å¯ç”¨èŠ‚æ—¥æ¨¡å¼ (è·ç¦»ä¸‹ä¸€èŠ‚æ—¥ {days_to_holiday} å¤©)")
    else:
        print(f"ğŸ“… æ ‡å‡†æ¨¡å¼ (è·ç¦»ä¸‹ä¸€èŠ‚æ—¥ {days_to_holiday} å¤©)")

    # è·å–åŸºç¡€æƒé‡å¹¶è¿›è¡ŒåŠ¨æ€è°ƒæ•´
    base_weights = get_base_weights()
    try:
        adjusted_weights = adjust_weights_for_available_fields(df, base_weights)
    except ValueError as e:
        print(f"âš ï¸ è·³è¿‡æ–‡ä»¶: {e}")
        return pd.DataFrame()

    # åº”ç”¨èŠ‚æ—¥æ¨¡å¼è°ƒæ•´ï¼ˆå¦‚æœå¯ç”¨ï¼‰
    if is_holiday_mode:
        adjusted_weights = adjust_holiday_weights(adjusted_weights, is_holiday_mode)

    # æ•°æ®ç±»å‹è½¬æ¢ï¼ˆå…ˆç¡®ä¿æ‰€æœ‰å¿…è¦å­—æ®µå­˜åœ¨ï¼‰
    numeric_cols = ['sales_7d', 'sales_30d', 'gmv_7d', 'gmv_30d',
                   'live_gmv_30d', 'live_gmv_7d', 'card_gmv_30d',
                   'sales_1y', 'conv_30d', 'commission', 'influencer_7d', 'rank_no']

    # è¡¥å…¨ç¼ºå¤±çš„æ•°å€¼å­—æ®µ
    for col in numeric_cols:
        if col not in df.columns:
            df[col] = 0
        else:
            df[col] = pd.to_numeric(df[col], errors='coerce').fillna(0)

    # è½¬åŒ–ç‡è¿‡æ»¤ï¼ˆä¼˜åŒ–è¿‡æ»¤é€»è¾‘ï¼‰
    if 'conv_30d' in df.columns and df['conv_30d'].notna().any():
        # åªæœ‰å½“è½¬åŒ–ç‡åˆ—å­˜åœ¨ä¸”æœ‰æœ‰æ•ˆæ•°æ®æ—¶æ‰è¿›è¡Œè¿‡æ»¤
        conv_score, valid_mask = filter_and_score_conversion(df['conv_30d'])

        # æ£€æŸ¥è¿‡æ»¤åæ˜¯å¦è¿˜æœ‰æ•°æ®
        if valid_mask.sum() == 0:
            print("âš ï¸ è½¬åŒ–ç‡è¿‡æ»¤è¿‡äºä¸¥æ ¼ï¼Œæ”¾å®½è¿‡æ»¤æ¡ä»¶")
            # æ”¾å®½è¿‡æ»¤æ¡ä»¶ï¼šè½¬åŒ–ç‡ >= 0.01 æˆ–ä½¿ç”¨æ‰€æœ‰æ•°æ®
            relaxed_mask = df['conv_30d'] >= 0.01
            if relaxed_mask.sum() > 0:
                df = df[relaxed_mask].copy()
                # é‡æ–°è®¡ç®—conv_scoreä»¥ç¡®ä¿ç´¢å¼•åŒ¹é…
                conv_score = df['conv_30d'].clip(0, 0.2) / 0.2 * 0.08
                print(f"ğŸ“Š ä½¿ç”¨æ”¾å®½æ¡ä»¶ï¼šè½¬åŒ–ç‡ >= 0.01ï¼Œä¿ç•™ {len(df)} è¡Œæ•°æ®")
            else:
                print("ğŸ“Š ä½¿ç”¨æ‰€æœ‰æ•°æ®ï¼Œä¸è¿›è¡Œè½¬åŒ–ç‡è¿‡æ»¤")
                # ä¸è¿‡æ»¤æ•°æ®ï¼Œé‡æ–°è®¡ç®—conv_score
                conv_score = df['conv_30d'].clip(0, 0.2) / 0.2 * 0.08
        else:
            df = df[valid_mask].copy()
            # é‡æ–°è®¡ç®—conv_scoreä»¥ç¡®ä¿ç´¢å¼•åŒ¹é…
            conv_score = df['conv_30d'].clip(0, 0.2) / 0.2 * 0.08
            print(f"ğŸ“Š è½¬åŒ–ç‡è¿‡æ»¤ï¼šä¿ç•™ {len(df)} è¡Œæ•°æ®ï¼ˆè½¬åŒ–ç‡ >= 0.02ï¼‰")
    else:
        print("ğŸ“Š è½¬åŒ–ç‡åˆ—ç¼ºå¤±æˆ–æ— æœ‰æ•ˆæ•°æ®ï¼Œä½¿ç”¨é»˜è®¤è½¬åŒ–ç‡è¯„åˆ†")
        conv_score = pd.Series(0.04, index=df.index)  # é»˜è®¤ä¸­ç­‰è½¬åŒ–ç‡è¯„åˆ†

    if len(df) == 0:
        print("âš ï¸ æ•°æ®è¿‡æ»¤åæ— æœ‰æ•ˆæ•°æ®")
        return pd.DataFrame()

    # è®¡ç®—å„ç»´åº¦å¾—åˆ†ï¼ˆä»…è®¡ç®—å­˜åœ¨çš„å­—æ®µï¼‰
    if 'sales_7d' in df.columns and adjusted_weights['sales_7d_score'] > 0:
        df['sales_7d_score'] = clip_and_normalize(df['sales_7d'])
    else:
        df['sales_7d_score'] = 0

    if 'sales_30d' in df.columns and adjusted_weights['sales_30d_score'] > 0:
        df['sales_30d_score'] = clip_and_normalize(df['sales_30d'])
    else:
        df['sales_30d_score'] = 0

    if 'gmv_7d' in df.columns and adjusted_weights['gmv_7d_score'] > 0:
        df['gmv_7d_score'] = clip_and_normalize(df['gmv_7d'])
    else:
        df['gmv_7d_score'] = 0

    if 'gmv_30d' in df.columns and adjusted_weights['gmv_30d_score'] > 0:
        df['gmv_30d_score'] = clip_and_normalize(df['gmv_30d'])
    else:
        df['gmv_30d_score'] = 0

    df['commission_score'] = score_commission(df['commission'])
    df['influencer_score'] = cosine_decay_score(df['influencer_7d'])
    df['rank_score'] = rank_score_with_decay(df['rank_type'], df['rank_no'], is_holiday_mode)

    # å¢é•¿æ½œåŠ›è¯„åˆ†éœ€è¦30å¤©æ•°æ®ï¼Œå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨7å¤©æ•°æ®ä¼°ç®—
    if 'sales_30d' in df.columns and 'sales_1y' in df.columns:
        df['growth_score'] = growth_potential_score(df['sales_30d'], df['sales_1y'])
    elif 'sales_7d' in df.columns and 'sales_1y' in df.columns:
        # ä½¿ç”¨7å¤©æ•°æ®ä¼°ç®—30å¤©æ•°æ®
        estimated_sales_30d = df['sales_7d'] * 4.3  # 30/7 â‰ˆ 4.3
        df['growth_score'] = growth_potential_score(estimated_sales_30d, df['sales_1y'])
    else:
        df['growth_score'] = 0.5  # é»˜è®¤ä¸­ç­‰å¢é•¿æ½œåŠ›

    # æ¸ é“åˆ†å¸ƒè¯„åˆ†ï¼ˆç°åœ¨æ”¯æŒç¼ºå¤±å­—æ®µï¼‰
    # ç¡®ä¿æ‰€æœ‰æ¸ é“ç›¸å…³å­—æ®µå­˜åœ¨ï¼Œç¼ºå¤±çš„ç”¨0å¡«å……
    for col in ['live_gmv_30d', 'live_gmv_7d', 'card_gmv_30d', 'gmv_7d']:
        if col not in df.columns:
            df[col] = 0

    df['channel_score'] = channel_distribution_score(
        df['live_gmv_30d'], df['live_gmv_7d'], df['card_gmv_30d'],
        df['gmv_30d'], df['gmv_7d']
    )

    df['conv_score'] = conv_score

    # ä½¿ç”¨è°ƒæ•´åçš„æƒé‡è®¡ç®—æ€»åˆ†
    df['total_score'] = calculate_total_score(df, adjusted_weights)

    print(f"âœ… æ–‡ä»¶å¤„ç†å®Œæˆ: {len(df)} è¡Œæœ‰æ•ˆæ•°æ®")

    return df

def calculate_total_score(df, weights):
    """è®¡ç®—åŠ æƒæ€»åˆ†ï¼ˆæ”¯æŒåŠ¨æ€æƒé‡ï¼‰"""
    total_score = pd.Series(0.0, index=df.index)

    for col, weight in weights.items():
        if col in df.columns and weight > 0:
            total_score += df[col] * weight

    return total_score

def process_data_files(input_dir, output_dir):
    """å¤„ç†æ‰€æœ‰æ•°æ®æ–‡ä»¶"""
    # æ‰«æè¾“å…¥ç›®å½•
    file_patterns = [
        os.path.join(input_dir, "*.csv"),
        os.path.join(input_dir, "*.xlsx")
    ]

    all_files = []
    for pattern in file_patterns:
        all_files.extend(glob.glob(pattern))

    if not all_files:
        print(f"åœ¨ç›®å½• {input_dir} ä¸­æœªæ‰¾åˆ°CSVæˆ–XLSXæ–‡ä»¶")
        return

    print(f"æ‰¾åˆ° {len(all_files)} ä¸ªæ–‡ä»¶å¾…å¤„ç†")

    all_results = []
    eliminated_count = 0

    for file_path in all_files:
        print(f"å¤„ç†æ–‡ä»¶: {os.path.basename(file_path)}")

        # è¯»å–æ•°æ®
        df = read_data_file(file_path)
        if df is None:
            continue

        original_count = len(df)

        # å¤„ç†æ•°æ®ï¼ˆæ—¥æœŸè§£æåœ¨å‡½æ•°å†…éƒ¨å®Œæˆï¼‰
        processed_df = process_single_file(df, None, False)  # ä¸´æ—¶ä¼ å…¥Noneï¼Œå‡½æ•°å†…éƒ¨ä¼šè§£æ

        if len(processed_df) == 0:
            print(f"  æ–‡ä»¶å¤„ç†åæ— æœ‰æ•ˆæ•°æ®")
            eliminated_count += original_count
            continue

        # æ€»åˆ†å·²åœ¨process_single_fileä¸­è®¡ç®—å®Œæˆ

        all_results.append(processed_df)
        eliminated_count += original_count - len(processed_df)

        print(f"  å¤„ç†å®Œæˆ: {len(processed_df)}/{original_count} è¡Œæœ‰æ•ˆ")

    if not all_results:
        print("æ²¡æœ‰æœ‰æ•ˆæ•°æ®å¯å¤„ç†")
        return

    # åˆå¹¶æ‰€æœ‰ç»“æœ
    combined_df = pd.concat(all_results, ignore_index=True)

    # æŒ‰product_urlå»é‡
    before_dedup = len(combined_df)
    combined_df = combined_df.drop_duplicates(subset=['product_url'], keep='first')
    after_dedup = len(combined_df)

    print(f"å»é‡å¤„ç†: {before_dedup} â†’ {after_dedup} è¡Œ")

    # æ’åºå–TOP50
    top50_df = combined_df.nlargest(50, 'total_score')

    # ä¿æŒåŸ17å­—æ®µ + total_score
    original_cols = ['product_name', 'product_url', 'category_l1', 'commission',
                    'sales_7d', 'gmv_7d', 'sales_30d', 'gmv_30d',
                    'live_gmv_30d', 'live_gmv_7d', 'card_gmv_30d',
                    'sales_1y', 'conv_30d', 'rank_type', 'rank_no',
                    'influencer_7d', 'snapshot_tag', 'file_date', 'data_period']

    output_cols = [col for col in original_cols if col in top50_df.columns] + ['total_score']
    top50_df = top50_df[output_cols]

    # è¾“å‡ºæ–‡ä»¶
    os.makedirs(output_dir, exist_ok=True)
    output_file = os.path.join(output_dir, "top50_combined.csv")
    top50_df.to_csv(output_file, index=False, encoding='utf-8-sig')

    print(f"\nå¤„ç†å®Œæˆ:")
    print(f"  å¤„ç†æ–‡ä»¶æ•°: {len(all_files)}")
    print(f"  æ·˜æ±°è¡Œæ•°: {eliminated_count}")
    print(f"  ç”Ÿæˆæ–‡ä»¶: {output_file}")
    print(f"  TOP50å•†å“å·²ä¿å­˜")

def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='ç”µå•†æ•°æ®æ™ºèƒ½è¿‡æ»¤è¯„åˆ†ç³»ç»Ÿ')
    parser.add_argument('--in', dest='input_dir', required=True, help='è¾“å…¥æ¸…æ´—æ•°æ®ç›®å½•')
    parser.add_argument('--out', dest='output_dir', required=True, help='è¾“å‡ºTOP50ç»“æœç›®å½•')

    args = parser.parse_args()

    if not os.path.exists(args.input_dir):
        print(f"è¾“å…¥ç›®å½•ä¸å­˜åœ¨: {args.input_dir}")
        return

    process_data_files(args.input_dir, args.output_dir)

if __name__ == "__main__":
    main()
